# Customer Churn Prediction - Jupyter Notebook
# Copy each cell below into separate cells in your Jupyter Notebook
# Each model is now independent with its own visualizations

# ============================================================================
# CELL 1: Import Basic Libraries
# ============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings
warnings.filterwarnings('ignore')

sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

print("‚úÖ Basic libraries imported successfully!")

# ============================================================================
# CELL 2: Load Dataset
# ============================================================================
print("=" * 80)
print("LOADING DATASET")
print("=" * 80)

df = pd.read_csv('Telco-Customer-Churn.csv')

print(f"\n‚úì Dataset loaded successfully!")
print(f"Dataset Shape: {df.shape}")
print(f"Number of Samples: {df.shape[0]}")
print(f"Number of Features: {df.shape[1]}")

print("\nFirst 5 rows:")
df.head()

# ============================================================================
# CELL 3: Data Exploration
# ============================================================================
print("=" * 80)
print("DATA EXPLORATION")
print("=" * 80)

print("\nDataset Information:")
df.info()

print("\nStatistical Summary:")
df.describe()

print("\nMissing Values:")
missing = df.isnull().sum()
if missing.sum() == 0:
    print("No missing values found!")
else:
    print(missing[missing > 0])

print("\nClass Distribution:")
print(df['Churn'].value_counts())
print("\nPercentage:")
print(df['Churn'].value_counts(normalize=True) * 100)

# ============================================================================
# CELL 4: Data Visualization
# ============================================================================
print("=" * 80)
print("DATA VISUALIZATION")
print("=" * 80)

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='Churn', palette='Set2')
plt.title('Customer Churn Distribution', fontsize=16, fontweight='bold')
plt.xlabel('Churn', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 8))
numerical_cols = df.select_dtypes(include=[np.number]).columns
correlation_matrix = df[numerical_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')
plt.title('Correlation Heatmap', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# ============================================================================
# CELL 5: Data Preprocessing
# ============================================================================
print("=" * 80)
print("DATA PREPROCESSING")
print("=" * 80)

df_processed = df.copy()

if 'customerID' in df_processed.columns:
    df_processed = df_processed.drop('customerID', axis=1)
    print("‚úì Removed customerID column")

if 'TotalCharges' in df_processed.columns:
    df_processed['TotalCharges'] = pd.to_numeric(df_processed['TotalCharges'], errors='coerce')
    df_processed['TotalCharges'].fillna(0, inplace=True)
    print("‚úì Converted TotalCharges and handled missing values")

binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']
for col in binary_cols:
    if col in df_processed.columns:
        df_processed[col] = df_processed[col].map({'Yes': 1, 'No': 0, 'Male': 1, 'Female': 0})
print("‚úì Encoded binary variables")

categorical_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
                   'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
                   'Contract', 'PaymentMethod']
df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)
print("‚úì Applied one-hot encoding")

df_processed['Churn'] = df_processed['Churn'].map({'Yes': 1, 'No': 0})
print("‚úì Encoded target variable")

print(f"\nProcessed Shape: {df_processed.shape}")
print(f"Features: {df_processed.shape[1] - 1}")

# ============================================================================
# CELL 6: Train-Test Split and Scaling (Moved to each model section)
# ============================================================================
print("=" * 80)
print("FEATURE PREPARATION")
print("=" * 80)

X = df_processed.drop('Churn', axis=1)
y = df_processed['Churn']

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print("\n‚úì Features and target separated")
print("\nNote: Train-test split and scaling will be done in each model section")

# ============================================================================
# CELL 7: Train ANN Model (Member 1) - Import Libraries
# ============================================================================
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve

print("‚úÖ ANN libraries imported!")

# ============================================================================
# CELL 8: Train ANN Model (Member 1) - Model Training
# ============================================================================
print("=" * 80)
print("MEMBER 1: ARTIFICIAL NEURAL NETWORK (ANN)")
print("=" * 80)

# Train-Test Split
print("\n1. Splitting data...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"   Training set: {X_train.shape[0]} samples")
print(f"   Test set: {X_test.shape[0]} samples")

# Feature Scaling
print("\n2. Scaling features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("   ‚úì StandardScaler applied")

# Save scaler for later use
with open('scaler.joblib', 'wb') as f:
    pickle.dump(scaler, f)
with open('feature_names.pkl', 'wb') as f:
    pickle.dump(X.columns.tolist(), f)
print("   ‚úì Scaler and feature names saved")

# Train ANN Model
print("\n3. Training ANN model...")
ann_model = MLPClassifier(
    hidden_layer_sizes=(100, 50),
    activation='relu',
    solver='adam',
    max_iter=500,
    random_state=42
)

ann_model.fit(X_train_scaled, y_train)
print("   ‚úì Training completed!")

# Predictions
y_pred_ann = ann_model.predict(X_test_scaled)
y_pred_proba_ann = ann_model.predict_proba(X_test_scaled)[:, 1]

# ============================================================================
# CELL 9: ANN Model - Evaluation Metrics
# ============================================================================
print("\n" + "=" * 80)
print("ANN MODEL - EVALUATION METRICS")
print("=" * 80)

ann_accuracy = accuracy_score(y_test, y_pred_ann)
ann_precision = precision_score(y_test, y_pred_ann)
ann_recall = recall_score(y_test, y_pred_ann)
ann_f1 = f1_score(y_test, y_pred_ann)
ann_auc = roc_auc_score(y_test, y_pred_proba_ann)

print(f"\nAccuracy:  {ann_accuracy:.4f}")
print(f"Precision: {ann_precision:.4f}")
print(f"Recall:    {ann_recall:.4f}")
print(f"F1-Score:  {ann_f1:.4f}")
print(f"AUC-ROC:   {ann_auc:.4f}")

# ============================================================================
# CELL 10: ANN Model - Metrics Visualization
# ============================================================================
print("\nüìä Generating ANN metrics visualization...")

fig, ax = plt.subplots(figsize=(10, 6))

metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']
metrics_values = [ann_accuracy, ann_precision, ann_recall, ann_f1, ann_auc]

bars = ax.bar(metrics_names, metrics_values, color='#FF6B6B', alpha=0.8, edgecolor='black')

ax.set_ylim([0, 1])
ax.set_ylabel('Score', fontsize=12, fontweight='bold')
ax.set_title('ANN Model - Performance Metrics', fontsize=16, fontweight='bold')
ax.grid(True, alpha=0.3, axis='y')

for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.4f}',
            ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

# ============================================================================
# CELL 11: ANN Model - Confusion Matrix
# ============================================================================
print("\nüìä Generating ANN confusion matrix...")

cm_ann = confusion_matrix(y_test, y_pred_ann)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_ann, annot=True, fmt='d', cmap='Reds', 
            xticklabels=['Not Churn', 'Churn'],
            yticklabels=['Not Churn', 'Churn'],
            cbar_kws={'label': 'Count'})
plt.title('ANN Model - Confusion Matrix', fontsize=16, fontweight='bold')
plt.ylabel('Actual', fontsize=12, fontweight='bold')
plt.xlabel('Predicted', fontsize=12, fontweight='bold')

# Add statistics
tn, fp, fn, tp = cm_ann.ravel()
stats_text = f'TN: {tn}  FP: {fp}\nFN: {fn}  TP: {tp}'
plt.text(1, -0.3, stats_text, fontsize=11, ha='center', 
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()

print(f"\nTrue Negatives:  {tn}")
print(f"False Positives: {fp}")
print(f"False Negatives: {fn}")
print(f"True Positives:  {tp}")

# ============================================================================
# CELL 12: ANN Model - ROC Curve
# ============================================================================
print("\nüìä Generating ANN ROC curve...")

fpr_ann, tpr_ann, thresholds_ann = roc_curve(y_test, y_pred_proba_ann)

plt.figure(figsize=(10, 8))
plt.plot(fpr_ann, tpr_ann, color='#FF6B6B', linewidth=3, 
         label=f'ANN (AUC = {ann_auc:.4f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')

plt.fill_between(fpr_ann, tpr_ann, alpha=0.3, color='#FF6B6B')

plt.xlabel('False Positive Rate', fontsize=13, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=13, fontweight='bold')
plt.title('ANN Model - ROC Curve', fontsize=16, fontweight='bold')
plt.legend(loc='lower right', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ============================================================================
# CELL 13: Save ANN Model
# ============================================================================
with open('ann_model.keras', 'wb') as f:
    pickle.dump(ann_model, f)
print("\n‚úì ANN model saved to 'ann_model.keras'")

print("\n" + "üéâ" * 40)
print("MEMBER 1 (ANN) - COMPLETED!")
print("üéâ" * 40)

# ============================================================================
# CELL 14: Train SVM Model (Member 2) - Import Libraries
# ============================================================================
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve

print("‚úÖ SVM libraries imported!")

# ============================================================================
# CELL 15: Train SVM Model (Member 2) - Model Training
# ============================================================================
print("=" * 80)
print("MEMBER 2: SUPPORT VECTOR MACHINE (SVM)")
print("=" * 80)

# Train-Test Split
print("\n1. Splitting data...")
X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"   Training set: {X_train_svm.shape[0]} samples")
print(f"   Test set: {X_test_svm.shape[0]} samples")

# Feature Scaling
print("\n2. Scaling features...")
scaler_svm = StandardScaler()
X_train_scaled_svm = scaler_svm.fit_transform(X_train_svm)
X_test_scaled_svm = scaler_svm.transform(X_test_svm)
print("   ‚úì StandardScaler applied")

# Train SVM Model
print("\n3. Training SVM model...")
svm_model = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',
    probability=True,
    random_state=42
)

svm_model.fit(X_train_scaled_svm, y_train_svm)
print("   ‚úì Training completed!")

# Predictions
y_pred_svm = svm_model.predict(X_test_scaled_svm)
y_pred_proba_svm = svm_model.predict_proba(X_test_scaled_svm)[:, 1]

# ============================================================================
# CELL 16: SVM Model - Evaluation Metrics
# ============================================================================
print("\n" + "=" * 80)
print("SVM MODEL - EVALUATION METRICS")
print("=" * 80)

svm_accuracy = accuracy_score(y_test_svm, y_pred_svm)
svm_precision = precision_score(y_test_svm, y_pred_svm)
svm_recall = recall_score(y_test_svm, y_pred_svm)
svm_f1 = f1_score(y_test_svm, y_pred_svm)
svm_auc = roc_auc_score(y_test_svm, y_pred_proba_svm)

print(f"\nAccuracy:  {svm_accuracy:.4f}")
print(f"Precision: {svm_precision:.4f}")
print(f"Recall:    {svm_recall:.4f}")
print(f"F1-Score:  {svm_f1:.4f}")
print(f"AUC-ROC:   {svm_auc:.4f}")

# ============================================================================
# CELL 17: SVM Model - Metrics Visualization
# ============================================================================
print("\nüìä Generating SVM metrics visualization...")

fig, ax = plt.subplots(figsize=(10, 6))

metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']
metrics_values = [svm_accuracy, svm_precision, svm_recall, svm_f1, svm_auc]

bars = ax.bar(metrics_names, metrics_values, color='#4ECDC4', alpha=0.8, edgecolor='black')

ax.set_ylim([0, 1])
ax.set_ylabel('Score', fontsize=12, fontweight='bold')
ax.set_title('SVM Model - Performance Metrics', fontsize=16, fontweight='bold')
ax.grid(True, alpha=0.3, axis='y')

for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.4f}',
            ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

# ============================================================================
# CELL 18: SVM Model - Confusion Matrix
# ============================================================================
print("\nüìä Generating SVM confusion matrix...")

cm_svm = confusion_matrix(y_test_svm, y_pred_svm)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Not Churn', 'Churn'],
            yticklabels=['Not Churn', 'Churn'],
            cbar_kws={'label': 'Count'})
plt.title('SVM Model - Confusion Matrix', fontsize=16, fontweight='bold')
plt.ylabel('Actual', fontsize=12, fontweight='bold')
plt.xlabel('Predicted', fontsize=12, fontweight='bold')

tn, fp, fn, tp = cm_svm.ravel()
stats_text = f'TN: {tn}  FP: {fp}\nFN: {fn}  TP: {tp}'
plt.text(1, -0.3, stats_text, fontsize=11, ha='center', 
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()

print(f"\nTrue Negatives:  {tn}")
print(f"False Positives: {fp}")
print(f"False Negatives: {fn}")
print(f"True Positives:  {tp}")

# ============================================================================
# CELL 19: SVM Model - ROC Curve
# ============================================================================
print("\nüìä Generating SVM ROC curve...")

fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test_svm, y_pred_proba_svm)

plt.figure(figsize=(10, 8))
plt.plot(fpr_svm, tpr_svm, color='#4ECDC4', linewidth=3, 
         label=f'SVM (AUC = {svm_auc:.4f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')

plt.fill_between(fpr_svm, tpr_svm, alpha=0.3, color='#4ECDC4')

plt.xlabel('False Positive Rate', fontsize=13, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=13, fontweight='bold')
plt.title('SVM Model - ROC Curve', fontsize=16, fontweight='bold')
plt.legend(loc='lower right', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ============================================================================
# CELL 20: Save SVM Model
# ============================================================================
with open('svm_model.joblib', 'wb') as f:
    pickle.dump(svm_model, f)
print("\n‚úì SVM model saved to 'svm_model.joblib'")

print("\n" + "üéâ" * 40)
print("MEMBER 2 (SVM) - COMPLETED!")
print("üéâ" * 40)

# ============================================================================
# CELL 21: Train KNN Model (Member 3) - Import Libraries
# ============================================================================
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve

print("‚úÖ KNN libraries imported!")

# ============================================================================
# CELL 22: Train KNN Model (Member 3) - Model Training
# ============================================================================
print("=" * 80)
print("MEMBER 3: K-NEAREST NEIGHBORS (KNN)")
print("=" * 80)

# Train-Test Split
print("\n1. Splitting data...")
X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"   Training set: {X_train_knn.shape[0]} samples")
print(f"   Test set: {X_test_knn.shape[0]} samples")

# Feature Scaling
print("\n2. Scaling features...")
scaler_knn = StandardScaler()
X_train_scaled_knn = scaler_knn.fit_transform(X_train_knn)
X_test_scaled_knn = scaler_knn.transform(X_test_knn)
print("   ‚úì StandardScaler applied")

# Train KNN Model
print("\n3. Training KNN model...")
knn_model = KNeighborsClassifier(
    n_neighbors=5,
    weights='distance',
    metric='euclidean'
)

knn_model.fit(X_train_scaled_knn, y_train_knn)
print("   ‚úì Training completed!")

# Predictions
y_pred_knn = knn_model.predict(X_test_scaled_knn)
y_pred_proba_knn = knn_model.predict_proba(X_test_scaled_knn)[:, 1]

# ============================================================================
# CELL 23: KNN Model - Evaluation Metrics
# ============================================================================
print("\n" + "=" * 80)
print("KNN MODEL - EVALUATION METRICS")
print("=" * 80)

knn_accuracy = accuracy_score(y_test_knn, y_pred_knn)
knn_precision = precision_score(y_test_knn, y_pred_knn)
knn_recall = recall_score(y_test_knn, y_pred_knn)
knn_f1 = f1_score(y_test_knn, y_pred_knn)
knn_auc = roc_auc_score(y_test_knn, y_pred_proba_knn)

print(f"\nAccuracy:  {knn_accuracy:.4f}")
print(f"Precision: {knn_precision:.4f}")
print(f"Recall:    {knn_recall:.4f}")
print(f"F1-Score:  {knn_f1:.4f}")
print(f"AUC-ROC:   {knn_auc:.4f}")

# ============================================================================
# CELL 24: KNN Model - Metrics Visualization
# ============================================================================
print("\nüìä Generating KNN metrics visualization...")

fig, ax = plt.subplots(figsize=(10, 6))

metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']
metrics_values = [knn_accuracy, knn_precision, knn_recall, knn_f1, knn_auc]

bars = ax.bar(metrics_names, metrics_values, color='#45B7D1', alpha=0.8, edgecolor='black')

ax.set_ylim([0, 1])
ax.set_ylabel('Score', fontsize=12, fontweight='bold')
ax.set_title('KNN Model - Performance Metrics', fontsize=16, fontweight='bold')
ax.grid(True, alpha=0.3, axis='y')

for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.4f}',
            ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

# ============================================================================
# CELL 25: KNN Model - Confusion Matrix
# ============================================================================
print("\nüìä Generating KNN confusion matrix...")

cm_knn = confusion_matrix(y_test_knn, y_pred_knn)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Greens', 
            xticklabels=['Not Churn', 'Churn'],
            yticklabels=['Not Churn', 'Churn'],
            cbar_kws={'label': 'Count'})
plt.title('KNN Model - Confusion Matrix', fontsize=16, fontweight='bold')
plt.ylabel('Actual', fontsize=12, fontweight='bold')
plt.xlabel('Predicted', fontsize=12, fontweight='bold')

tn, fp, fn, tp = cm_knn.ravel()
stats_text = f'TN: {tn}  FP: {fp}\nFN: {fn}  TP: {tp}'
plt.text(1, -0.3, stats_text, fontsize=11, ha='center', 
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()

print(f"\nTrue Negatives:  {tn}")
print(f"False Positives: {fp}")
print(f"False Negatives: {fn}")
print(f"True Positives:  {tp}")

# ============================================================================
# CELL 26: KNN Model - ROC Curve
# ============================================================================
print("\nüìä Generating KNN ROC curve...")

fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test_knn, y_pred_proba_knn)

plt.figure(figsize=(10, 8))
plt.plot(fpr_knn, tpr_knn, color='#45B7D1', linewidth=3, 
         label=f'KNN (AUC = {knn_auc:.4f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')

plt.fill_between(fpr_knn, tpr_knn, alpha=0.3, color='#45B7D1')

plt.xlabel('False Positive Rate', fontsize=13, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=13, fontweight='bold')
plt.title('KNN Model - ROC Curve', fontsize=16, fontweight='bold')
plt.legend(loc='lower right', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ============================================================================
# CELL 27: Save KNN Model
# ============================================================================
with open('knn_model.joblib', 'wb') as f:
    pickle.dump(knn_model, f)
print("\n‚úì KNN model saved to 'knn_model.joblib'")

print("\n" + "üéâ" * 40)
print("MEMBER 3 (KNN) - COMPLETED!")
print("üéâ" * 40)

# ============================================================================
# CELL 28: Final Comparison - All Models
# ============================================================================
print("\n" + "=" * 80)
print("FINAL COMPARISON - ALL MODELS")
print("=" * 80)

results = {
    'ANN': {
        'Accuracy': ann_accuracy,
        'Precision': ann_precision,
        'Recall': ann_recall,
        'F1-Score': ann_f1,
        'AUC-ROC': ann_auc
    },
    'SVM': {
        'Accuracy': svm_accuracy,
        'Precision': svm_precision,
        'Recall': svm_recall,
        'F1-Score': svm_f1,
        'AUC-ROC': svm_auc
    },
    'KNN': {
        'Accuracy': knn_accuracy,
        'Precision': knn_precision,
        'Recall': knn_recall,
        'F1-Score': knn_f1,
        'AUC-ROC': knn_auc
    }
}

results_df = pd.DataFrame(results).T
print("\n", results_df)

# ============================================================================
# CELL 29: Comparison - Metrics Bar Chart
# ============================================================================
print("\nüìä Generating comparison chart...")

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
fig.suptitle('Model Performance Comparison - All Metrics', fontsize=18, fontweight='bold')

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']
models = list(results.keys())
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']

for idx, metric in enumerate(metrics):
    row = idx // 3
    col = idx % 3
    
    values = [results[model][metric] for model in models]
    bars = axes[row, col].bar(models, values, color=colors, alpha=0.8, edgecolor='black')
    axes[row, col].set_title(metric, fontsize=14, fontweight='bold')
    axes[row, col].set_ylim([0, 1])
    axes[row, col].set_ylabel('Score', fontsize=11)
    axes[row, col].grid(True, alpha=0.3, axis='y')
    
    for bar in bars:
        height = bar.get_height()
        axes[row, col].text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

fig.delaxes(axes[1, 2])
plt.tight_layout()
plt.show()

# ============================================================================
# CELL 30: Comparison - Confusion Matrices
# ============================================================================
print("\nüìä Generating comparison confusion matrices...")

fig, axes = plt.subplots(1, 3, figsize=(18, 5))
fig.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold')

cms = [cm_ann, cm_svm, cm_knn]
model_names = ['ANN', 'SVM', 'KNN']
cmaps = ['Reds', 'Blues', 'Greens']

for idx, (cm, name, cmap) in enumerate(zip(cms, model_names, cmaps)):
    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, ax=axes[idx],
                xticklabels=['Not Churn', 'Churn'],
                yticklabels=['Not Churn', 'Churn'])
    axes[idx].set_title(f'{name}', fontsize=14, fontweight='bold')
    axes[idx].set_ylabel('Actual', fontsize=12)
    axes[idx].set_xlabel('Predicted', fontsize=12)

plt.tight_layout()
plt.show()

# ============================================================================
# CELL 31: Comparison - ROC Curves
# ============================================================================
print("\nüìä Generating comparison ROC curves...")

plt.figure(figsize=(10, 8))

roc_data = [
    (fpr_ann, tpr_ann, 'ANN', ann_auc, '#FF6B6B'),
    (fpr_svm, tpr_svm, 'SVM', svm_auc, '#4ECDC4'),
    (fpr_knn, tpr_knn, 'KNN', knn_auc, '#45B7D1')
]

for fpr, tpr, name, auc, color in roc_data:
    plt.plot(fpr, tpr, color=color, linewidth=3, label=f'{name} (AUC = {auc:.4f})')

plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')

plt.xlabel('False Positive Rate', fontsize=13, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=13, fontweight='bold')
plt.title('ROC Curves Comparison - All Models', fontsize=16, fontweight='bold')
plt.legend(loc='lower right', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# ============================================================================
# CELL 32: Best Model Selection
# ============================================================================
print("\n" + "=" * 80)
print("BEST MODEL SELECTION")
print("=" * 80)

best_model_name = max(results, key=lambda x: results[x]['F1-Score'])
best_f1 = results[best_model_name]['F1-Score']

if best_model_name == 'ANN':
    best_model_obj = ann_model
elif best_model_name == 'SVM':
    best_model_obj = svm_model
else:
    best_model_obj = knn_model

print(f"\nüèÜ Best Model: {best_model_name}")
print(f"üìä F1-Score: {best_f1:.4f}")
print("\nDetailed Performance:")
for metric, value in results[best_model_name].items():
    print(f"  {metric}: {value:.4f}")

# ============================================================================
# CELL 33: Project Summary
# ============================================================================
print("\n" + "=" * 80)
print("PROJECT SUMMARY")
print("=" * 80)
print(f"üìÅ Dataset: {len(df)} customers")
print(f"üî¢ Original Features: 21")
print(f"üî¢ Processed Features: {len(X.columns)}")
print(f"ü§ñ Models Trained: 3 (ANN, SVM, KNN)")
print(f"\nüèÜ Best Model: {best_model_name}")
print(f"üìä Best F1-Score: {best_f1:.4f}")

print("\nüì¶ Saved Files:")
print("  ‚úÖ ann_model.keras")
print("  ‚úÖ svm_model.joblib")
print("  ‚úÖ knn_model.joblib")
print("  ‚úÖ scaler.joblib")
print("  ‚úÖ feature_names.pkl")

print("\n" + "=" * 80)
print("üéâ ALL ANALYSIS COMPLETED SUCCESSFULLY! üéâ")
print("=" * 80)
print("\n‚úÖ Ready to use in Streamlit app!")
print("   Run: streamlit run app.py")